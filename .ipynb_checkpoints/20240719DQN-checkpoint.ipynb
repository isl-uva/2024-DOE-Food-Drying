{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b2ef352-41e4-452c-85b9-312afeaad693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Tuple\n",
    "from pyfmi import load_fmu\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = DymolaEnv()\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f2387cc-1677-4c0d-831e-79611700984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DymolaEnv(Env):\n",
    "    def __init__(self):\n",
    "        # define action space\n",
    "        self.action_space = Tuple((\n",
    "            Box(low=np.array([0.25]), high=np.array([0.8]), dtype=np.float32),\n",
    "            # Agent 0: Flow_r, Regeneration air flowrate\n",
    "            Box(low=np.array([1]), high=np.array([100]), dtype=np.float32),\n",
    "            # Agent 1: N, Rotation speed of Desiccant wheel\n",
    "            Box(low=np.array([273.15]), high=np.array([353.15]), dtype=np.float32),\n",
    "        # Agent 2: Tset2, Temperature setpoint of Heater 2\n",
    "        ))\n",
    "        # define observation space\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([1]), dtype=np.float32)\n",
    "\n",
    "        # load fmu file\n",
    "        self.model = load_fmu(\"DWHP1_fmu 1.fmu\", kind='cs', log_level=2)\n",
    "\n",
    "        self.output = 30  ## numbers of outputs\n",
    "        self.start_time = 0  #\n",
    "        self.end_time = 10 * 3600  #\n",
    "        self.current_time = self.start_time\n",
    "        self.step_size = 300  # Define your own step size, unit is second, 300s = 5min\n",
    "        self.done = False\n",
    "        self.energy = 0\n",
    "        self.num_agents = len(self.action_space)\n",
    "        self.power_ref = 6000.0\n",
    "        self.Temp_d = 273.15 + 46\n",
    "        self.RH_d = 0.1\n",
    "\n",
    "        # Create a list with 30 elements, all of which are 0, using iteration\n",
    "        zero_list = []\n",
    "        for _ in range(self.output):\n",
    "            zero_list.append(0)\n",
    "\n",
    "        self.state = zero_list\n",
    "        # Initialize the model with the start time\n",
    "        self.model.reset()\n",
    "        self.model.initialize(self.start_time)\n",
    "        print(\"success boot\")\n",
    "\n",
    "        # set start temp\n",
    "        # self.state=np.array([38+random.uniform(-3, 3), 38+random.uniform(-3, 3), 38+random.uniform(-3, 3)])\n",
    "        # Temparture, Mo\n",
    "        # self.state=np.array([25.0, 1.0])\n",
    "        # set shower length\n",
    "        self.time_id = 0\n",
    "        self.dt = 0.2\n",
    "        self.error_thres = 0.001\n",
    "\n",
    "    def step(self, action):\n",
    "        # set action parameters\n",
    "        paras = action\n",
    "        self.model.set('FLOW_p', paras[0])\n",
    "        self.model.set('FLOW_r', paras[1])\n",
    "        self.model.set('N', paras[2])\n",
    "        self.model.set('SP_HP', paras[3])\n",
    "        self.model.set('Tset1', paras[4])\n",
    "        self.model.set('Tset2', paras[5])\n",
    "\n",
    "        self.model.do_step(self.current_time, self.step_size, True)\n",
    "\n",
    "        # Caluculate Observation\n",
    "        observation_all = []\n",
    "\n",
    "        for i in range(self.output):\n",
    "            self.state[i] = self.model.get(f\"y{i}\")[0]\n",
    "            if i >= 6 and i % 3 != 2:\n",
    "                observation_all.append(self.state[i])\n",
    "\n",
    "        # Caluculate Observation\n",
    "        observation_all = []\n",
    "\n",
    "        for i in range(self.output):\n",
    "            self.state[i] = self.model.get(f\"y{i}\")[0]\n",
    "            if i >= 6 and i % 3 != 2:\n",
    "                observation_all.append(self.state[i])\n",
    "\n",
    "        observation_all.append(self.state[1])\n",
    "        observation_all.append(self.state[2])\n",
    "        observation_all.append(self.state[5])\n",
    "\n",
    "    \n",
    "        self.obs_env = np.array(observation_all)\n",
    "        \n",
    "        #Update Time\n",
    "        self.current_time += self.step_size\n",
    "\n",
    "        # Check if time is larger than terminate time setting\n",
    "        if self.current_time >= self.end_time:\n",
    "            self.done = True\n",
    "            self.model.terminate()\n",
    "            print(\"dymola model end\")\n",
    "        else:\n",
    "            self.done = False\n",
    "\n",
    "        # Apply temperature noise\n",
    "        # self.state += random.randint(-1,1)\n",
    "        # Set placeholder for info\n",
    "        Temp_d = self.Temp_d\n",
    "        RH_d = self.RH_d\n",
    "\n",
    "        Temp_fc = self.state[27]\n",
    "        RH_fc = self.state[28]\n",
    "\n",
    "        humidity_error = abs(RH_d - RH_fc)\n",
    "        temp_error = abs(Temp_d - Temp_fc)\n",
    "\n",
    "        # calculate power\n",
    "        power = 0\n",
    "        for i in range(6):\n",
    "            power += self.state[i]\n",
    "\n",
    "        self.energy += power * self.step_size\n",
    "\n",
    "        # calculate reward\n",
    "        if humidity_error < 0.005:\n",
    "            hum_reward = 1\n",
    "        else:\n",
    "            hum_reward = -1\n",
    "\n",
    "        if temp_error < 1:\n",
    "            temp_reward = 1\n",
    "        else:\n",
    "            temp_reward = -1\n",
    "\n",
    "        power_reward = (self.power_ref - power) / self.power_ref\n",
    "\n",
    "        reward_all = hum_reward + temp_reward + power_reward\n",
    "\n",
    "        reward = reward_all\n",
    "        done = self.done\n",
    "    \n",
    "\n",
    "        info = {}\n",
    "\n",
    "        # return step information\n",
    "        return self.obs_env, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        print(self.state)\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize the model with the start time\n",
    "\n",
    "        if self.current_time >= self.end_time:\n",
    "            self.model = load_fmu(\"DWHP1_fmu 1.fmu\", kind='cs', log_level=2)\n",
    "            self.model.reset()\n",
    "            self.model.initialize(self.start_time)\n",
    "            self.current_time = 0\n",
    "            self.energy = 0\n",
    "            self.done = False\n",
    "            print(\"Normal end: time reset\")\n",
    "\n",
    "        if self.current_time>0:\n",
    "            if not self.done:\n",
    "                self.model.terminate()\n",
    "                self.model = load_fmu(\"DWHP1_fmu 1.fmu\", kind='cs', log_level=2)\n",
    "                self.model.reset()\n",
    "                self.model.initialize(self.start_time)\n",
    "                self.current_time = 0\n",
    "                self.energy = 0\n",
    "                self.done = False\n",
    "                print(\"Error end: time reset\")\n",
    "\n",
    "\n",
    "\n",
    "        # Caluculate Observation\n",
    "        observation_all = []\n",
    "\n",
    "        for i in range(self.output):\n",
    "            self.state[i] = self.model.get(f\"y{i}\")[0]\n",
    "            if i >= 6 and i % 3 != 2:\n",
    "                observation_all.append(self.state[i])\n",
    "\n",
    "        observation_all.append(self.state[1])\n",
    "        observation_all.append(self.state[2])\n",
    "        observation_all.append(self.state[5])\n",
    "\n",
    "    \n",
    "        self.obs_env = np.array(observation_all)\n",
    "\n",
    "        info = []\n",
    "\n",
    "\n",
    "        print(\"reset activate\")\n",
    "        return self.obs_env, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b058caf5-d656-4c68-be81-3132a1f1ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8821e32-25ac-415c-870e-5e9b3444e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "219d500a-71c3-4b16-8f24-9a2c13a72766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = len(env.action_space)\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(6000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d7b528-bcdd-44fc-bf3b-7ab1007d2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        print(\"no learning\")\n",
    "        return\n",
    "    print(\"start learning\")\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a17f94f5-4756-4db4-9e20-84c421935be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: <class 'numpy.ndarray'>,[ 0.02746198 -0.03521593  0.03115232 -0.00927336]\n",
      "count(0)\n",
      "0\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0275, -0.0352,  0.0312, -0.0093]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.02675766  0.15944573  0.03096686 -0.29196686]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "1\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0268,  0.1594,  0.0310, -0.2920]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.02994658  0.35411277  0.02512752 -0.5747246 ]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "2\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0299,  0.3541,  0.0251, -0.5747]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.03702883  0.5488736   0.01363302 -0.859387  ]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "3\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0370,  0.5489,  0.0136, -0.8594]], device='cuda:0')\n",
      "action: <class 'int'>,0\n",
      "observation: <class 'numpy.ndarray'>,[ 0.04800631  0.35356864 -0.00355471 -0.56244874]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "4\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0480,  0.3536, -0.0036, -0.5624]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.05507768  0.54874027 -0.01480369 -0.85624945]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "5\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0551,  0.5487, -0.0148, -0.8562]], device='cuda:0')\n",
      "action: <class 'int'>,0\n",
      "observation: <class 'numpy.ndarray'>,[ 0.06605248  0.35382316 -0.03192868 -0.5682579 ]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "6\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0661,  0.3538, -0.0319, -0.5683]], device='cuda:0')\n",
      "action: <class 'int'>,0\n",
      "observation: <class 'numpy.ndarray'>,[ 0.07312895  0.15916325 -0.04329384 -0.28580233]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "7\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0731,  0.1592, -0.0433, -0.2858]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.07631221  0.35487506 -0.04900988 -0.59181935]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "8\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0763,  0.3549, -0.0490, -0.5918]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.08340971  0.5506476  -0.06084627 -0.89952874]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "9\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0834,  0.5506, -0.0608, -0.8995]], device='cuda:0')\n",
      "action: <class 'int'>,0\n",
      "observation: <class 'numpy.ndarray'>,[ 0.09442267  0.3564007  -0.07883684 -0.62657535]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "10\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.0944,  0.3564, -0.0788, -0.6266]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.10155068  0.55252945 -0.09136835 -0.94300866]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "11\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.1016,  0.5525, -0.0914, -0.9430]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.11260127  0.74875575 -0.11022852 -1.2629454 ]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "12\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.1126,  0.7488, -0.1102, -1.2629]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.12757638  0.9451008  -0.13548744 -1.588017  ]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "13\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.1276,  0.9451, -0.1355, -1.5880]], device='cuda:0')\n",
      "action: <class 'int'>,1\n",
      "observation: <class 'numpy.ndarray'>,[ 0.1464784   1.1415476  -0.16724777 -1.9196982 ]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "14\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.1465,  1.1415, -0.1672, -1.9197]], device='cuda:0')\n",
      "action: <class 'int'>,0\n",
      "observation: <class 'numpy.ndarray'>,[ 0.16930935  0.94857246 -0.20564173 -1.6832166 ]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "15\n",
      "state: <class 'torch.Tensor'>,tensor([[ 0.1693,  0.9486, -0.2056, -1.6832]], device='cuda:0')\n",
      "action: <class 'int'>,0\n",
      "observation: <class 'numpy.ndarray'>,[ 0.1882808   0.7563393  -0.23930606 -1.4609758 ]\n",
      "reward: <class 'torch.Tensor'>tensor([1.], device='cuda:0')\n",
      "no learning\n",
      "break\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 1\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    print(f\"state: {type(state)},{state}\")\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    print(count())\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "        print(t)\n",
    "        print(f\"state: {type(state)},{state}\")\n",
    "        print(f\"action: {type(action.item())},{action.item()}\")\n",
    "        print(f\"observation: {type(observation)},{observation}\")\n",
    "        print(f\"reward: {type(reward)}{reward}\")\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            print('break')\n",
    "            #plot_durations()\n",
    "            break\n",
    "\n",
    "#print('Complete')\n",
    "#plot_durations(show_result=True)\n",
    "#plt.ioff()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d44984f3-dc64-49a7-9759-d9761b36ab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d59b6f-0012-41ea-88a9-514ba5ec9963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a0d6e9-8796-45e2-8043-1dbd133b803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "651226e6-863e-48e4-befb-e7cfb86311d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "print(A.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ea250cb-53de-4e55-b91b-dfbed67d2d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54192006, 0.08770577])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48fb3e46-efd0-416c-88ae-c18a219aa47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Matrix:\n",
      "[ 1.32018871  1.07587971  0.79650708 -0.90382416  0.17051513]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a 5x5 matrix from the standard normal distribution\n",
    "matrix = np.random.randn(5)\n",
    "\n",
    "# Print the generated matrix\n",
    "print(\"Generated Matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "# Plotting the distribution of the matrix values\n",
    "plt.hist(matrix.flatten(), bins=30, alpha=0.75)\n",
    "plt.title(\"Distribution of Matrix Values\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aff488e6-9e70-436c-a318-ad5c1e9cb334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: [-0.098006]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dim = 4\n",
    "action_dim = 1\n",
    "hidden_size = 256\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_dim),\n",
    "            nn.Tanh()  # assuming actions are scaled to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat([state, action], dim=-1))\n",
    "\n",
    "class GaussianNoise:\n",
    "    def __init__(self, action_dimension, scale=0.2):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.scale = scale\n",
    "\n",
    "    def noise(self):\n",
    "        return np.random.normal(0, self.scale, self.action_dimension)\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_size):\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_size).to(device)\n",
    "        self.critic = Critic(state_dim, action_dim, hidden_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.noise = GaussianNoise(action_dim)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(state).cpu().data.numpy()\n",
    "        self.actor.train()\n",
    "        return action + self.noise.noise()\n",
    "\n",
    "# Example usage\n",
    "agent = DDPGAgent(state_dim, action_dim, hidden_size)\n",
    "state = np.random.randn(state_dim)\n",
    "action = agent.select_action(state)\n",
    "print(\"Sampled action:\", action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8270fd90-508d-43cd-9e61-7d466a410547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.noise.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e0272a2-08a7-4225-8f67-7c7c3de6cc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.noise.action_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c028253-7faa-4b30-b1ba-be8baec1f510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
